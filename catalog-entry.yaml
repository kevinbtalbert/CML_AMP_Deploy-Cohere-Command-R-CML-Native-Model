name: Deploy Cohere Command R Model for Inference

entries:
  - title: Deploy Cohere Command R Model for Inference
    label: llm-model-deploy
    short_description: |
      This AMP deploys Cohere's Command R model as a CML API endpoint. Requires a GPU node with 4 vCores and 16 GB memory minimum.
    long_description: |
      This AMP deploys Cohere's Command R model as a CML API endpoint. Requires a GPU node with 4 vCores and 16 GB memory minimum.
      Note the following site settings prerequisite: Go to Site Administration > Settings > Ephemeral Storage Limit (in GB) and ensure value is set to at least 20GB
    image_path: "https://raw.githubusercontent.com/kevinbtalbert/CML_AMP_Deploy-Cohere-Command-R-CML-Native-Model/main/images/catalog-entry.jpg"
    tags: 
      - Cohere
      - Command R
      - LLM
      - Model Deployment
    git_url: "https://github.com/kevinbtalbert/CML_AMP_Deploy-Cohere-Command-R-CML-Native-Model.git"
    is_prototype: true
